Here’s the content from the image, formatted in text for your Word document:

TCoE Value Streams

The TCoE framework is structured around four primary value streams (VS), each supported by sub-value streams (SVS) and corresponding objectives (O). These value streams are aligned with strategic goals to ensure seamless integration and operational excellence.

VS1: Establishing a Robust TCoE Ecosystem
	•	SVS1.1: Stakeholder Engagement and Current Ecosystem Analysis
	•	O1: Identify and engage key stakeholders.
	•	O2: Assess the current TCoE ecosystem.
	•	SVS1.2: Operating Model Development and Team Building
	•	O1: Design an enhanced TCoE operating model.
	•	O2: Build a high-performing TCoE team.

VS2: Continuous Advancement
	•	SVS2.1: Global Test Package Creation and Test Script Management
	•	O1: Establish a suite of automated global test packages.
	•	O2: Set up centralized test script management.
	•	SVS2.2: Best Practices and Lessons Learned
	•	O1: Embed standardized best practices and integrate lessons learned.

VS3: Building Trusted Partnerships and User Support
	•	SVS3.1: Stakeholder Engagement and Alignment
	•	O1: Introduce TCoE to stakeholders.
	•	O2: Align sub-value streams (SVS) with stakeholder needs.
	•	SVS3.2: Contingency Planning and User Support
	•	O1: Establish contingency plans for critical scenarios.
	•	O2: Deliver effective user training and support.

VS4: Future-Readiness and Continuous Improvement
	•	SVS4.1: Addressing Current System Limitations and Data Quality Issues
	•	O1: Identify and document current system limitations and data quality issues to drive awareness and prioritization.
	•	SVS4.2: Future State Deployment
	•	O1: Enable future state architecture for seamless integration and operation.

You can copy and paste this structured text into your Word document. Let me know if you need further refinements!







Here’s the revised introduction with subheadings and bullet points for better structure and richness:

Introduction to the Testing Center of Excellence (TCOE)

The Testing Center of Excellence (TCOE) is a structured framework designed to enable organizations to deliver reliable, scalable, and innovative solutions. By aligning with operational and regulatory priorities, TCOE acts as a trusted partner to finance and risk teams, driving excellence in testing and operational efficiency.

Purpose and Ambition
	•	Purpose: To deliver fit-for-purpose and reliable solutions that empower finance and risk teams to achieve operational and regulatory excellence.
	•	Ambition: Serve as a trusted advisor by leveraging a robust testing ecosystem to enhance data value, accessibility, and visualization.

Core Strategies of TCOE
	1.	Robust Testing Ecosystem:
	•	Define a comprehensive Testing Target Operating Model (TOM) aligning with upstream, downstream, and DAO processes.
	•	Build a skilled, multifaceted team to manage diverse testing needs.
	•	Design and implement automated test packages for functional, integration, and regression testing.
	2.	Trusted Partnerships:
	•	Create deep understanding of stakeholders’ needs and partner to resolve challenges.
	•	Collaborate to innovate and deliver consistent solutions for all.
	3.	Continuous Advancement:
	•	Leverage TCOE as an R&D lab to foster innovation and automation.
	•	Standardize test cases for quality assurance and future-proofing.
	•	Incorporate lessons learned from past issues to improve future testing.
	4.	Aspire for a New World:
	•	Simulate processes for risk and finance using automation to enhance efficiency.
	•	Scale processes to meet evolving organizational needs.
	•	Embrace new technologies to achieve operational excellence.

Values Driving TCOE
	•	Manage Risk: Strengthen test coverage and proactively identify risks.
	•	Automation through Innovation: Aspire to automate and standardize testing to enhance operational efficiency.
	•	Insightful Controls: Collaborate with stakeholders to deliver integrated, innovative solutions.
	•	Future-Proofing: Develop scalable and customizable testing strategies to meet diverse requirements.

This framework establishes TCOE as a cornerstone of excellence, ensuring a strategic approach to testing while empowering organizations to navigate complex challenges with confidence.

You can now add this directly to your Word document. Let me know if you need further refinements!






Here is the text from the image:

5 Scrum Events
	•	Sprint Event
	•	Heartbeat of Scrum
	•	2-4 weeks per sprint
	•	Shorter sprints limit risk of cost and effort
	•	No changes made
	•	Sprint Planning
	•	Team attends
	•	Select items from backlog for sprint
	•	Decide Definition of Done (D.O.D) for increments
	•	Daily Standup
	•	15 mins/day
	•	Team attends
	•	Create actionable plan for next day
	•	3 questions asked by Scrum Master
	•	Sprint Review
	•	Last event in sprint
	•	Inspect outcome of sprint
	•	Present results to shareholders
	•	Collaborate on what to do next
	•	Sprint Retrospective
	•	Discuss what went well and where to improve with team
	•	Plan ways to increase quality and effectiveness

4 Agile Values
	1.	Individuals and Interactions over processes and tools
	2.	Working Software over comprehensive documentation
	3.	Customer Collaboration over contract negotiation
	4.	Responding to Change over following a plan

4 Scrum Accountabilities
	1.	Scrum Master
	•	A facilitator and coach for the Scrum team.
	•	Supports team, product owner, and organization.
	2.	Product Owner
	•	Responsible for defining and prioritizing the product backlog.
	•	Tasked with maximizing the value delivered by the team.
	3.	Developers
	•	Self-organizing and cross-functional team of subject matter experts.
	4.	Scrum Team
	•	A team of 10 or less including a Product Owner, Scrum Master, and Developers.

......
Here is the updated section incorporating pivoting and unpivoting columns as a complexity parameter:

3.2 Complexity Parameters

The following parameters are used to estimate the complexity of the process:
	1.	Number of Data Enrichment Files:
	•	The number of input files used for lookup, filtering, or enrichment.
	2.	Allocation Rules:
	•	Whether allocation or proportion logic is required.
	3.	Aggregation Requirements:
	•	Steps needed to consolidate data for the final output.
	4.	Control Requirements:
	•	Number and type of validations/reconciliations in pre- and post-control books.
	5.	Mapping Tables:
	•	The number of mapping tables used for lookups and enrichment.
	6.	Pivoting and Unpivoting Columns:
	•	The requirement to pivot or unpivot data columns based on specific categorical columns.
	•	Time estimation: Each column pivoted or unpivoted will take approximately 15 minutes.

5. Estimation Calculation Table

The updated calculation table, including pivoting/unpivoting complexity, is as follows:

Activity	Unit of Work	Time per Unit (minutes)	Comments
Column Copying	Per column	10	Simple copy operation.
Lookup for Enrichment	Per column	20	Includes mapping table lookups.
Filtering	Per condition	15	Based on specific criteria.
Allocation	Per rule	25	Allocation or proportion calculations.
Aggregation for Uploader	Per dataset	40	Includes consolidation steps.
Reconciliation (Post-Control)	Per control point	30	Data comparison and validation.
Pivoting/Unpivoting Columns	Per column	15	Based on specific categorical columns.

This update reflects the inclusion of pivoting/unpivoting columns as a distinct complexity parameter, with a clear time estimate. Let me know if you’d like further refinements!



-----

Estimation Methodology for Solution Development Using Rule and Control Books

Abstract

The purpose of this paper is to develop a structured methodology for estimating the time required to create solutions involving rule books and control books for data adjustment processes. The estimation framework considers inputs from domain experts, the complexity of the process, and detailed operations required for data transformation and validation. This methodology provides a systematic approach to time estimation by breaking down the process into manageable components and assigning time estimates based on defined complexity parameters.

1. Introduction

In data processing and adjustment workflows, creating solutions for automating manual processes involves detailed rule definitions and control frameworks. These solutions typically comprise a rule book that outlines data transformation rules and control books to validate data integrity at both pre- and post-transformation stages. Estimating the time required to design and implement such solutions is a critical step to plan resources and manage stakeholder expectations effectively. This paper outlines a methodology to estimate development time based on the complexity and nature of tasks involved.

2. Objective

The objective of this paper is to define a methodology for estimating the time required to:
	1.	Write the rules in the rule book, including data enrichment, filtering, aggregation, and calculations.
	2.	Develop pre- and post-control books for data validation and reconciliation.
	3.	Quantify the impact of key complexity parameters on the time estimation process.

3. Methodology

3.1 Process Overview

	1.	Expert Consultation:
	•	Domain experts manually performing the data adjustment process will provide detailed explanations of the workflow.
	•	Inputs include:
	•	Input files used (transaction data, mapping files, filters, etc.).
	•	Specific rules for copying, enriching, filtering, and calculating data.
	•	Allocation and aggregation rules to create final output.
	2.	Rule Book Creation:
	•	Documenting data processing rules, such as:
	•	Identifying input columns for copying.
	•	Lookup operations for data enrichment.
	•	Filters applied for specific conditions.
	•	Allocation and proportion rules.
	•	Aggregation and transformation steps.
	3.	Control Book Creation:
	•	Pre-Control Book:
	•	Validations on raw input data (e.g., null value checks, specific mappings, or range validations).
	•	Post-Control Book:
	•	Validations and reconciliations on output data, including:
	•	Comparing aggregated outputs with input data.
	•	Verifying consolidated values.

3.2 Complexity Parameters

The following parameters are used to estimate the complexity of the process:
	1.	Number of Data Enrichment Files:
	•	The number of input files used for lookup, filtering, or enrichment.
	2.	Allocation Rules:
	•	Whether allocation or proportion logic is required.
	3.	Aggregation Requirements:
	•	Steps needed to consolidate data for the final output.
	4.	Control Requirements:
	•	Number and type of validations/reconciliations in pre- and post-control books.
	5.	Mapping Tables:
	•	The number of mapping tables used for lookups and enrichment.

3.3 Time Estimation Framework

Each activity is assigned a standard time estimate based on its complexity. For example:
	•	Column Copying: 10 minutes per column.
	•	Lookup Operations: 20 minutes per column.
	•	Reconciliation Tasks: 30 minutes per control.
	•	Pivoting/Unpivoting Data: 30 minutes per operation.

Time estimates will be summed based on the total number of operations identified during the expert consultation phase.

4. Assumptions

The following assumptions underpin the methodology:
	1.	The domain expert can articulate the process steps clearly without ambiguities.
	2.	Input files and their schemas are available and well-documented.
	3.	Time estimates are based on average processing times observed in prior implementations.
	4.	Any unforeseen complexities (e.g., missing input data) will be addressed through additional consultation, impacting the time estimate.

5. Estimation Calculation Table

The following table outlines time estimates for individual tasks based on complexity parameters:

Activity	Unit of Work	Time per Unit (minutes)	Comments
Column Copying	Per column	10	Simple copy operation.
Lookup for Enrichment	Per column	20	Includes mapping table lookups.
Filtering	Per condition	15	Based on specific criteria.
Allocation	Per rule	25	Allocation or proportion calculations.
Aggregation for Uploader	Per dataset	40	Includes consolidation steps.
Reconciliation (Post-Control)	Per control point	30	Data comparison and validation.
Pivoting/Unpivoting Columns	Per operation	30	Reformatting data structure.

6. Conclusion

This paper provides a structured approach for estimating the time required to create rule books and control books for data adjustment processes. The methodology incorporates expert inputs, defined complexity parameters, and standard time estimates for each activity. This framework can be adapted for similar projects to ensure accurate and reliable time planning.


-----
import os
import shutil
import re
from openpyxl import load_workbook

# Define the original and temporary folder paths
original_folder = 'path/to/original/folder'
temp_folder = 'path/to/temporary/folder'

# Define the list of keywords to search within formulas
search_keywords = ["getValue", "retrieveData", "calculateTotal"]

# Compile regex patterns for each keyword to find matches in formulas
patterns = {keyword: re.compile(rf"\b{re.escape(keyword)}\b", re.IGNORECASE) for keyword in search_keywords}

# Create a temporary folder to store non-password-protected copies
if not os.path.exists(temp_folder):
    os.makedirs(temp_folder)
    print(f"Created temporary folder at '{temp_folder}'")
else:
    print(f"Temporary folder '{temp_folder}' already exists")

# Copy all files to the temporary folder, keeping the folder structure
print("Starting file copy process...")
for root, _, files in os.walk(original_folder):
    for file_name in files:
        # Check if the file is an Excel file
        if file_name.endswith('.xlsx') or file_name.endswith('.xlsm'):
            # Construct full file paths
            src = os.path.join(root, file_name)
            
            # Preserve folder structure in the temporary folder
            relative_path = os.path.relpath(root, original_folder)
            dest_dir = os.path.join(temp_folder, relative_path)
            if not os.path.exists(dest_dir):
                os.makedirs(dest_dir)
                print(f"Created directory '{dest_dir}' in temporary folder")

            dest = os.path.join(dest_dir, file_name)
            
            # Copy the file to the corresponding location in the temporary folder
            shutil.copy(src, dest)
            print(f"Copied '{file_name}' to temporary folder")

print("File copy process completed. Starting search for keywords...")

# Dictionary to store files containing any of the keywords, with the matched keyword(s)
files_with_keywords = {}

# Now iterate through the copied files in the temporary folder
for root, _, files in os.walk(temp_folder):
    for file_name in files:
        file_path = os.path.join(root, file_name)
        print(f"Checking file '{file_name}' for keywords {search_keywords}...")

        # Try opening the workbook, skipping if it's password-protected
        try:
            workbook = load_workbook(file_path, data_only=False)  # Load formulas as they are
            print(f"Opened workbook '{file_name}' successfully")
        except Exception as e:
            print(f"Could not open '{file_name}' due to password protection or other error.")
            continue

        # Search for each keyword in formulas in each sheet
        found_keywords = set()
        for sheet in workbook.sheetnames:
            print(f"Searching in sheet '{sheet}' of '{file_name}'...")
            worksheet = workbook[sheet]
            for row in worksheet.iter_rows():
                for cell in row:
                    if cell.data_type == 'f':  # Only check cells with formulas
                        cell_value = str(cell.value)
                        # Check each pattern to see if any keyword matches
                        for keyword, pattern in patterns.items():
                            if pattern.search(cell_value):
                                found_keywords.add(keyword)
                                print(f"Keyword '{keyword}' found in '{file_name}' in sheet '{sheet}'")
                                break  # Stop checking other keywords for this cell
                if found_keywords:
                    break  # Stop further search in this sheet if a keyword is found
            if found_keywords:
                break  # Stop further search in this file if a keyword is found
        workbook.close()

        # If any keywords were found, update the results dictionary
        if found_keywords:
            files_with_keywords[file_name] = list(found_keywords)
            print(f"File '{file_name}' updated in the results list with matched keywords: {found_keywords}")

print("Search completed.")

# Output the results
if files_with_keywords:
    print("\nThe following files contain formulas with the specified keywords:")
    for file, keywords in files_with_keywords.items():
        print(f"File: {file} - Matched Keywords: {', '.join(keywords)}")
else:
    print("No files containing the specified keywords were found.")
