import pandas as pd

# Sample dataframe
data = {
    'Category': ['A', 'A', 'B', 'B', 'C', 'C'],
    'Type': ['X', 'Y', 'X', 'Y', 'X', 'Y'],
    'Value': [10, 20, 15, 25, 10, 30]
}
df = pd.DataFrame(data)

# Groupby and pivot operations
grouped = df.groupby(['Category', 'Type']).sum().reset_index()
pivoted = df.pivot_table(index='Category', columns='Type', values='Value', aggfunc='sum').reset_index()

# Create Excel file with views
with pd.ExcelWriter('control_sheet.xlsx') as writer:
    df.to_excel(writer, sheet_name='Original Data', index=False)
    grouped.to_excel(writer, sheet_name='Grouped Data', index=False)
    pivoted.to_excel(writer, sheet_name='Pivoted Data', index=False)

----

from pptx import Presentation
from pptx.util import Inches
import pandas as pd

# Function to add a dataframe to a slide
def add_table_to_slide(slide, dataframe, title):
    rows, cols = dataframe.shape
    table = slide.shapes.add_table(rows + 1, cols, Inches(0.5), Inches(1.5), Inches(9.0), Inches(0.8 * (rows + 1))).table

    # Set title
    title_shape = slide.shapes.title
    title_shape.text = title

    # Set column headings
    for i, column in enumerate(dataframe.columns):
        table.cell(0, i).text = column

    # Add the data
    for row in range(rows):
        for col in range(cols):
            table.cell(row + 1, col).text = str(dataframe.iat[row, col])

# Create presentation object
prs = Presentation()

# Titles for slides
titles = ['Original Data', 'Grouped Data', 'Pivoted Data']
comments = ['This is the original dataset.', 'Data grouped by Category and Type.', 'Pivot table of the data.']

# Load data from the Excel file
with pd.ExcelFile('control_sheet.xlsx') as xls:
    for title, comment in zip(titles, comments):
        slide = prs.slides.add_slide(prs.slide_layouts[5])
        df = pd.read_excel(xls, sheet_name=title)
        add_table_to_slide(slide, df, title)
        
        # Add comments
        textbox = slide.shapes.add_textbox(Inches(0.5), Inches(5.5), Inches(9.0), Inches(1.0))
        text_frame = textbox.text_frame
        p = text_frame.add_paragraph()
        p.text = comment

# Save the presentation
prs.save('control_presentation.pptx')




---/

import openpyxl
import pandas as pd

# Function to unmerge cells and keep the value in the first cell
def unmerge_cells_to_dataframe(excel_file):
    # Load the Excel file
    wb = openpyxl.load_workbook(excel_file)
    ws = wb.active

    # Collect all merged cell ranges
    merged_ranges = list(ws.merged_cells.ranges)

    for merged_range in merged_ranges:
        merged_range_str = str(merged_range)
        top_left_cell = merged_range_str.split(":")[0]
        top_left_cell_value = ws[top_left_cell].value
        ws.unmerge_cells(merged_range_str)
        ws[top_left_cell].value = top_left_cell_value

    # Save the updated Excel file temporarily
    updated_excel_file = 'unmerged_excel_temp.xlsx'
    wb.save(updated_excel_file)

    # Load the unmerged Excel file into a Pandas DataFrame
    df = pd.read_excel(updated_excel_file)

    return df

# Specify the path to your Excel file
excel_file = 'path_to_your_excel_file.xlsx'

# Get the DataFrame after unmerging cells
df_unmerged = unmerge_cells_to_dataframe(excel_file)

print("DataFrame after unmerging cells:")
print(df_unmerged)

------





import openpyxl
import pandas as pd

# Load the Excel file
excel_file = 'path_to_your_excel_file.xlsx'
wb = openpyxl.load_workbook(excel_file)
ws = wb.active

# Function to unmerge cells and keep the value in the first cell
def unmerge_cells(ws):
    # Collect all merged cell ranges
    merged_ranges = list(ws.merged_cells.ranges)
    
    for merged_range in merged_ranges:
        merged_range_str = str(merged_range)
        top_left_cell = merged_range_str.split(":")[0]
        top_left_cell_value = ws[top_left_cell].value
        ws.unmerge_cells(merged_range_str)
        ws[top_left_cell].value = top_left_cell_value

# Unmerge cells in the active sheet
unmerge_cells(ws)

# Save the updated Excel file
updated_excel_file = 'unmerged_excel_file.xlsx'
wb.save(updated_excel_file)

# Load the unmerged Excel file into a Pandas DataFrame
df = pd.read_excel(updated_excel_file)

print("DataFrame after unmerging cells:")
print(df)

----



import openpyxl
import pandas as pd

# Load the Excel file
excel_file = 'path_to_your_excel_file.xlsx'
wb = openpyxl.load_workbook(excel_file)
ws = wb.active

# Function to unmerge cells and keep the value in the first cell
def unmerge_cells(ws):
    merged_cells = ws.merged_cells.ranges
    for merged_range in merged_cells:
        merged_range_str = str(merged_range)
        top_left_cell_value = ws[merged_range_str.split(":")[0]].value
        ws.unmerge_cells(merged_range_str)
        ws[merged_range_str.split(":")[0]] = top_left_cell_value

# Unmerge cells in the active sheet
unmerge_cells(ws)

# Save the updated Excel file
updated_excel_file = 'unmerged_excel_file.xlsx'
wb.save(updated_excel_file)

# Load the unmerged Excel file into a Pandas DataFrame
df = pd.read_excel(updated_excel_file)

print("DataFrame after unmerging cells:")
print(df)

------




import pandas as pd

# Sample DataFrame
data = {
    'Col1': [None, 'A', None, 'D'],
    'Col2': ['B', None, 'C', None],
    'Col3': [None, None, 'E', 'F'],
    'Col4': ['G', 'H', None, 'I']
}

df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Function to shift non-null values to the left
def shift_left(row):
    non_nulls = [x for x in row if pd.notnull(x)]
    nulls = [x for x in row if pd.isnull(x)]
    return non_nulls + nulls

# Apply the function to each row
df_shifted = df.apply(shift_left, axis=1, result_type='expand')

print("\nDataFrame after shifting values to the left:")
print(df_shifted)

-----


import fitz  # PyMuPDF
import csv

# Function to extract text from PDF and save to a CSV
def pdf_to_csv(pdf_path, csv_path):
    # Open the PDF
    pdf_document = fitz.open(pdf_path)
    
    # Create a list to hold the lines of text
    text_lines = []
    
    # Iterate through each page
    for page_num in range(pdf_document.page_count):
        page = pdf_document[page_num]
        text = page.get_text("text")
        # Split the text into lines and extend the list
        text_lines.extend(text.splitlines())
    
    # Remove blank lines
    text_lines = [line for line in text_lines if line.strip()]

    # Write the text lines to a CSV file
    with open(csv_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        for line in text_lines:
            writer.writerow([line])

# Example usage
pdf_path = 'path/to/your/file.pdf'
csv_path = 'path/to/your/output.csv'
pdf_to_csv(pdf_path, csv_path)




-----


import pandas as pd
import numpy as np

# Sample data
data = {
    'column1': ['text1', '1,500', '', '3,000', 'text2', ''],
    'column2': ['2', 'text3', '2,500.50', '', '5', ''],
    'description': ['', 'desc1', '', 'desc2', '', 'desc3']
}

# Creating DataFrame
df = pd.DataFrame(data)

def preprocess_value(value):
    # Check if the value is a string and contains a comma
    if isinstance(value, str) and ',' in value:
        # Remove commas and convert to float
        value = value.replace(',', '')
        try:
            value = float(value)
        except ValueError:
            pass  # Keep the original value if it cannot be converted to float
    return value

# Apply preprocessing to the columns
df['column1'] = df['column1'].apply(preprocess_value)
df['column2'] = df['column2'].apply(preprocess_value)

def concat_or_add(row):
    val1, val2 = row['column1'], row['column2']
    
    if (pd.isna(val1) or val1 == '') and (pd.isna(val2) or val2 == ''):
        return 'N.A.'
    
    if pd.isna(val1) or val1 == '':
        val1 = 0 if isinstance(val2, (int, float)) else ''
    if pd.isna(val2) or val2 == '':
        val2 = 0 if isinstance(val1, (int, float)) else ''
        
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        return val1 + val2
    else:
        return str(val1) + str(val2)

df['column3'] = df.apply(concat_or_add, axis=1)

# Filter out rows where column3 contains only integer or float values and where the description is not blank
filtered_df = df[df['column3'].apply(lambda x: isinstance(x, (int, float))) & (df['description'].apply(pd.notna) & (df['description'] != ''))]

# Reset index for the filtered DataFrame
filtered_df.reset_index(drop=True, inplace=True)

print(filtered_df)




--------

import pandas as pd
import numpy as np

# Sample data
data = {
    'column1': ['text1', 1.5, '', 3, 'text2', ''],
    'column2': [2, 'text3', 2.5, '', 5, ''],
    'description': ['', 'desc1', '', 'desc2', '', 'desc3']
}

# Creating DataFrame
df = pd.DataFrame(data)

def concat_or_add(row):
    val1, val2 = row['column1'], row['column2']
    
    if (pd.isna(val1) or val1 == '') and (pd.isna(val2) or val2 == ''):
        return 'N.A.'
    
    if pd.isna(val1) or val1 == '':
        val1 = 0 if isinstance(val2, (int, float)) else ''
    if pd.isna(val2) or val2 == '':
        val2 = 0 if isinstance(val1, (int, float)) else ''
        
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        return val1 + val2
    else:
        return str(val1) + str(val2)

df['column3'] = df.apply(concat_or_add, axis=1)

# Filter out rows where both columns contain float values and where the description is not blank
filtered_df = df[(df['column1'].apply(lambda x: isinstance(x, (int, float))) | df['column2'].apply(lambda x: isinstance(x, (int, float)))) & (df['description'] != '')]

# Reset index for the filtered DataFrame
filtered_df.reset_index(drop=True, inplace=True)

print(filtered_df)



----/

import pandas as pd
import numpy as np

# Sample data
data = {
    'column1': ['text1', 1.5, '', 3, 'text2'],
    'column2': [2, 'text3', 2.5, '', 5]
}

# Creating DataFrame
df = pd.DataFrame(data)

def concat_or_add(row):
    val1, val2 = row['column1'], row['column2']
    
    if pd.isna(val1) or val1 == '':
        val1 = 0 if isinstance(val2, (int, float)) else ''
    if pd.isna(val2) or val2 == '':
        val2 = 0 if isinstance(val1, (int, float)) else ''
        
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        return val1 + val2
    else:
        return str(val1) + str(val2)

df['column3'] = df.apply(concat_or_add, axis=1)

print(df)





----------

import pandas as pd

def process_invoice_excel(input_excel_path, output_excel_path, column_names):
    # Read the Excel file starting from row 12
    df = pd.read_excel(input_excel_path, skiprows=11, header=None)
    
    # Set the column names explicitly
    df.columns = column_names

    # Remove completely blank rows
    df.dropna(how='all', inplace=True)

    # Remove rows with no relevant data
    df.dropna(subset=[column_names[0]], inplace=True)  # Assuming the first column is mandatory

    # Find the index of the "subtotal" row
    subtotal_index = df[df.apply(lambda row: row.astype(str).str.contains('subtotal', case=False).any(), axis=1)].index

    # If "subtotal" is found, keep rows only before "subtotal"
    if not subtotal_index.empty:
        df = df.loc[:subtotal_index[0] - 1]

    # Reset index for the cleaned DataFrame
    df.reset_index(drop=True, inplace=True)

    # Save the cleaned DataFrame to a new Excel file
    df.to_excel(output_excel_path, index=False)

# Example usage
input_excel_path = 'input_invoice.xlsx'
output_excel_path = 'cleaned_invoice_items.xlsx'
column_names = ["Description", "Quantity", "Price", "Total"]  # Adjust these as needed
process_invoice_excel(input_excel_path, output_excel_path, column_names)





----------

import pandas as pd

def process_invoice_excel(input_excel_path, output_excel_path):
    # Read the Excel file starting from row 12
    df = pd.read_excel(input_excel_path, skiprows=11)

    # Remove completely blank rows
    df.dropna(how='all', inplace=True)

    # Remove rows with no relevant data
    # Assuming 'Description' column is one of the mandatory fields for a valid row
    df.dropna(subset=['Description'], inplace=True)

    # Find the index of the "subtotal" row
    subtotal_index = df[df.apply(lambda row: row.astype(str).str.contains('subtotal', case=False).any(), axis=1)].index

    # If "subtotal" is found, keep rows only before "subtotal"
    if not subtotal_index.empty:
        df = df.loc[:subtotal_index[0] - 1]

    # Reset index for the cleaned DataFrame
    df.reset_index(drop=True, inplace=True)

    # Save the cleaned DataFrame to a new Excel file
    df.to_excel(output_excel_path, index=False)

# Example usage
input_excel_path = 'input_invoice.xlsx'
output_excel_path = 'cleaned_invoice_items.xlsx'
process_invoice_excel(input_excel_path, output_excel_path)










------------------

import pandas as pd

# Sample original data frame
data = {
    'Account': ['A001', 'A002', 'A003', 'A004', 'A005'],
    'Affiliate': ['Affiliate_1', 'Affiliate_None', 'Affiliate_2', 'Affiliate_None', 'Affiliate_3'],
    'Amount': [100, 200, 300, 400, 500]
}
df = pd.DataFrame(data)

# Reference table with account validity
reference_data = {
    'Account': ['A001', 'A002', 'A003', 'A004', 'A005'],
    'IsValid': ['y', 'y', 'n', 'y', 'n']
}
reference_df = pd.DataFrame(reference_data)

# Merge the original data frame with the reference table
merged_df = pd.merge(df, reference_df, on='Account')

# Filter the accounts where IsValid is 'y' and Affiliate is 'Affiliate_None'
invalid_affiliates = merged_df[(merged_df['IsValid'] == 'y') & (merged_df['Affiliate'] == 'Affiliate_None')]

# Get the list of such accounts
invalid_accounts = invalid_affiliates['Account'].tolist()

# Display the list of invalid accounts
print("List of accounts with invalid affiliates:", invalid_accounts)



---------

import os
import pandas as pd

# Example mapping dataframe
mapping_df = pd.DataFrame({
    'EntityId': [1, 2, 3, 4, 5, 'common'],
    'EntityCode': ['A', 'B', 'C', 'D', 'E', 'X']
})

# Path to the main folder containing subfolders with country dataframes
main_folder_path = 'path_to_main_folder'

# Loop through each subfolder
for country_folder in os.listdir(main_folder_path):
    country_folder_path = os.path.join(main_folder_path, country_folder)
    
    if os.path.isdir(country_folder_path):
        # Assuming each subfolder contains a CSV file with the country dataframe
        for file_name in os.listdir(country_folder_path):
            if file_name.endswith('.csv'):
                file_path = os.path.join(country_folder_path, file_name)
                
                # Load the country dataframe
                country_df = pd.read_csv(file_path)
                
                # Filter the mapping dataframe based on EntityId in country dataframe
                entity_ids_in_country = country_df['EntityId'].unique()
                filtered_mapping_df = mapping_df[mapping_df['EntityId'].isin(entity_ids_in_country) | (mapping_df['EntityId'] == 'common')]
                
                # Process the filtered mapping dataframe as needed
                print(f"Filtered mapping dataframe for {country_folder}:")
                print(filtered_mapping_df)

                # If you need to save or further process the filtered_mapping_df, you can do so here

-----------


import pandas as pd
import numpy as np

# Example dataframe
data = {'column': [np.nan, 'abc', np.nan, np.nan, 'abc']}
df = pd.DataFrame(data)

# Identify the single non-null value in the column
unique_values = df['column'].dropna().unique()

# Create the control DataFrame without NULL values
control_df = df.dropna().copy()

# Determine the message for control DataFrame
if len(unique_values) == 1:
    control_df['status'] = 'Unique value found'
    single_value = unique_values[0]
    # Fill NaN values with the single value in the original DataFrame
    df['column'].fillna(single_value, inplace=True)
else:
    control_df['status'] = 'More than one unique non-null value or none at all'

print("Original DataFrame after filling NaNs:")
print(df)
print("\nControl DataFrame:")
print(control_df)

---/--




import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId1_main': [1, 2, 3, 4, 5], 'EntityId2_main': [10, 20, 30, 40, 50], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId1': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key1': [2, 3, 4], 'Key2': [20, 30, 40], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, {'main': ['EntityId1_main'], 'map': ['EntityId1']}),
    ('df2', df2, {'main': ['EntityId1_main', 'EntityId2_main'], 'map': ['Key1', 'Key2']}),
    ('df3', df3, {'main': ['EntityId1_main'], 'map': ['Id']}),
    ('df4', df4, {'main': ['EntityId1_main'], 'map': ['Code']}),
    ('df5', df5, {'main': ['EntityId1_main'], 'map': ['ID']})
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, keys in mapping_dfs:
    main_keys = keys['main']
    map_keys = keys['map']
    merged_df = main_df.merge(mapping_df, how='left', left_on=main_keys, right_on=map_keys, indicator=True)
    # Identify the rows in main_df that don't have matching rows in the mapping DataFrame
    unmatched = merged_df[merged_df['_merge'] == 'left_only'][main_keys]
    for value in unmatched.itertuples(index=False, name=None):
        unmatched_values_list.append([name, ', '.join(main_keys), ', '.join(map(str, value))])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")





import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId1_main': [1, 2, 3, 4, 5], 'EntityId2_main': [10, 20, 30, 40, 50], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId1': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key1': [2, 3, 4], 'Key2': [20, 30, 40], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, {'main': ['EntityId1_main'], 'map': ['EntityId1']}),
    ('df2', df2, {'main': ['EntityId1_main', 'EntityId2_main'], 'map': ['Key1', 'Key2']}),
    ('df3', df3, {'main': ['EntityId1_main'], 'map': ['Id']}),
    ('df4', df4, {'main': ['EntityId1_main'], 'map': ['Code']}),
    ('df5', df5, {'main': ['EntityId1_main'], 'map': ['ID']})
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, keys in mapping_dfs:
    main_keys = keys['main']
    map_keys = keys['map']
    merged_df = main_df.merge(mapping_df, how='left', left_on=main_keys, right_on=map_keys, indicator=True)
    # Identify the rows in main_df that don't have matching rows in the mapping DataFrame
    unmatched = merged_df[merged_df['_merge'] == 'left_only'][main_keys]
    for value in unmatched.itertuples(index=False):
        unmatched_values_list.append([name, ', '.join(main_keys), value])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")

----------




import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId1': [1, 2, 3, 4, 5], 'EntityId2': [10, 20, 30, 40, 50], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId1': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key1': [2, 3, 4], 'Key2': [20, 30, 40], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'Details': [10, 20, 30, 40, 50]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'Description': [1, 2, 3, 4, 5]})

mapping_dfs = [
    ('df1', df1, ['EntityId1']),
    ('df2', df2, ['Key1', 'Key2']),
    ('df3', df3, ['Id']),
    ('df4', df4, ['Code']),
    ('df5', df5, ['ID'])
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, keys in mapping_dfs:
    merged_df = main_df.merge(mapping_df, how='left', left_on=keys, right_on=keys)
    # Identify the rows in main_df that don't have matching rows in the mapping DataFrame
    unmatched = merged_df[merged_df[keys[0]].isna()][keys]
    for value in unmatched.itertuples(index=False):
        unmatched_values_list.append([name, ', '.join(keys), tuple(value)])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")

----//////



import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId': [1, 2, 3, 4, 5], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key': [2, 3, 4], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, 'EntityId'),
    ('df2', df2, 'Key'),
    ('df3', df3, 'Id'),
    ('df4', df4, 'Code'),
    ('df5', df5, 'ID')
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, key in mapping_dfs:
    merged_df = main_df.merge(mapping_df, how='left', left_on='EntityId', right_on=key)
    unmatched = merged_df[merged_df[key].isna()]['EntityId']
    for value in unmatched:
        unmatched_values_list.append([name, key, value])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")

-------------




import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId': [1, 2, 3, 4, 5], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key': [2, 3, 4], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, 'EntityId'),
    ('df2', df2, 'Key'),
    ('df3', df3, 'Id'),
    ('df4', df4, 'Code'),
    ('df5', df5, 'ID')
]

# Create a summary count table
summary_counts = []

# Create a dictionary to store unmatched values
unmatched_values = {name: [] for name, _, _ in mapping_dfs}

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, key in mapping_dfs:
    merged_df = main_df.merge(mapping_df, how='left', left_on='EntityId', right_on=key)
    unmatched = merged_df[merged_df[key].isna()]['EntityId']
    unmatched_values[name].extend(unmatched.tolist())
    summary_counts.append([name, len(unmatched)])

# Create a summary DataFrame
summary_df = pd.DataFrame(summary_counts, columns=['DataFrame', 'Unmatched Count'])

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the summary count table
    summary_df.to_excel(writer, sheet_name='Summary', index=False)
    
    # Write the unmatched values to separate worksheets
    for name, values in unmatched_values.items():
        if values:
            unmatched_df = pd.DataFrame(values, columns=['Unmatched EntityId'])
            unmatched_df.to_excel(writer, sheet_name=f'Unmatched_{name}', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")




-----------


import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames and their names and key columns
df1 = pd.DataFrame({'EntityId': [1, 2, 3, 4, 1], 'Value': [10, 20, 30, 40, 50]})
df2 = pd.DataFrame({'Key1': [5, 6, 7, 8, 5], 'Key2': [1, 2, 3, 4, 1], 'Data': [100, 200, 300, 400, 500]})
df3 = pd.DataFrame({'Id': [9, 10, 11, 12, 12], 'Info': [1000, 2000, 3000, 4000, 5000]})
df4 = pd.DataFrame({'Code': [13, 14, 15, 16, 13], 'Details': [10, 20, 30, 40, 50]})
df5 = pd.DataFrame({'ID': [17, 18, 19, 20, 20], 'Description': [1, 2, 3, 4, 5]})

dataframes = [
    ('df1', df1, ['EntityId']),
    ('df2', df2, ['Key1', 'Key2']),
    ('df3', df3, ['Id']),
    ('df4', df4, ['Code']),
    ('df5', df5, ['ID'])
]

# Consolidated results DataFrame
consolidated_results = []

# Create a new Excel writer object
with ExcelWriter('duplicates_only.xlsx') as writer:
    for name, df, keys in dataframes:
        # Find duplicate rows
        duplicates = df[df.duplicated(subset=keys, keep=False)]
        
        # Count the number of unique items where the total count is greater than 1
        unique_counts = df.groupby(keys).size()
        duplicate_count = unique_counts[unique_counts > 1].count()
        
        # Append the results to the consolidated list
        consolidated_results.append([name, ', '.join(keys), duplicate_count])
        
        # Write the duplicates to the Excel sheet if any
        if not duplicates.empty:
            duplicates.to_excel(writer, sheet_name=f'duplicates_{name}', index=False)

    # Create a DataFrame from the consolidated results
    consolidated_df = pd.DataFrame(consolidated_results, columns=['DataFrame', 'Key Columns', 'Duplicate Count'])
    
    # Write the consolidated results to the Excel sheet
    consolidated_df.to_excel(writer, sheet_name='Consolidated Results', index=False)

print("Duplicates and consolidated results have been written to 'duplicates_only.xlsx'.")

-----------///////------





import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames and their key columns
df1 = pd.DataFrame({'EntityId': [1, 2, 3, 4, 1], 'Value': [10, 20, 30, 40, 50]})
df2 = pd.DataFrame({'Key': [5, 6, 7, 8, 5], 'Data': [100, 200, 300, 400, 500]})
df3 = pd.DataFrame({'Id': [9, 10, 11, 12, 12], 'Info': [1000, 2000, 3000, 4000, 5000]})
df4 = pd.DataFrame({'Code': [13, 14, 15, 16, 13], 'Details': [10, 20, 30, 40, 50]})
df5 = pd.DataFrame({'ID': [17, 18, 19, 20, 20], 'Description': [1, 2, 3, 4, 5]})

dataframes = [
    (df1, 'EntityId'),
    (df2, 'Key'),
    (df3, 'Id'),
    (df4, 'Code'),
    (df5, 'ID')
]

# Create a new Excel writer object
with ExcelWriter('data_with_duplicates.xlsx') as writer:
    for idx, (df, key) in enumerate(dataframes, start=1):
        # Check for duplicates
        duplicates = df[df.duplicated(key, keep=False)]
        
        # Write the original DataFrame to the Excel sheet
        df.to_excel(writer, sheet_name=f'DataFrame{idx}', index=False)
        
        # Write the duplicates to the Excel sheet if any
        if not duplicates.empty:
            duplicates.to_excel(writer, sheet_name=f'Duplicates{idx}', index=False)

print("DataFrames and duplicates have been written to 'data_with_duplicates.xlsx'.")








import pandas as pd

# Sample DataFrame with string and float columns
data = {
    'col1': [1, 2, 3],
    'col2': [4.5, 5.5, 6.5],  # Column to keep all decimals
    'col3': [7, 8, 9],
    'col4': [10, 11, 12],
    'col5': ['A', 'B', 'C'],  # String column
    'col6': [16, 17, 18],
    'col7': ['X', 'Y', 'Z'],  # String column
    'col8': [22, 23, 24],
    'col9': [25, 26, 27],
    'col10': [28.5, 29.5, 30.5]  # Column to round to zero decimals
}

df = pd.DataFrame(data)

# File path
file_path = 'output.txt'

# Header string
header = "My data is here:"

# Specify the column names
float_col_to_round = 'col10'
float_col_keep_decimals = 'col2'

# Write to file with header
with open(file_path, 'w') as file:
    file.write(f'{header}\n')
    for index, row in df.iterrows():
        formatted_values = []
        for col in df.columns:
            value = row[col]
            if col == float_col_to_round:
                formatted_values.append(f'{value:.0f}')  # Round to zero decimals
            else:
                formatted_values.append(f'{value}')
        line = ';'.join(formatted_values)
        file.write(f'{line}\n')

# Display the content of the text file for verification
with open(file_path, 'r') as file:
    print(file.read())





import pandas as pd

# Example DataFrame
df = pd.DataFrame({
    'column2': ['A', 'B', 'C', 'D', 'A', 'F']
})

# Example mapping DataFrame
mapping_df = pd.DataFrame({
    'column2': ['B', 'C', 'D'],
    'column3': ['MappedValue1', 'MappedValue2', 'MappedValue3']
})

# Define a function to apply the logic
def populate_column1(row, mapping_dict):
    if row['column2'] == 'A':
        return 'b'
    elif row['column2'] in mapping_dict:
        return mapping_dict[row['column2']]
    else:
        return 'e'

# Create a dictionary for faster lookup from the mapping DataFrame
mapping_dict = mapping_df.set_index('column2')['column3'].to_dict()

# Apply the function to populate 'column1'
df['column1'] = df.apply(populate_column1, axis=1, mapping_dict=mapping_dict)

print(df)








= let
    Value = [Value],
    Currency = [Currency],
    RangeStart = Number.RoundDown(Value / 50000) * 50000,
    RangeEnd = RangeStart + 49999,
    RangeText = if Value < 5000 then Currency & " less than " & Currency & " 5000" else Currency & " " & Text.From(RangeStart + 1) & " to " & Currency & " " & Text.From(RangeEnd)
in
    RangeText



import pandas as pd

# Sample data for the DataFrame
data = {
    'Categories': ['A', 'B', 'C', 'D'],
    'Values': [10, 20, 30, 40]
}

df = pd.DataFrame(data)

# Create a Pandas Excel writer using XlsxWriter as the engine
with pd.ExcelWriter('data_with_chart.xlsx', engine='xlsxwriter') as writer:
    # Write your DataFrame to an Excel file on Sheet1
    df.to_excel(writer, sheet_name='Sheet1', index=False)

    # Access the XlsxWriter workbook and worksheet objects from the dataframe
    workbook = writer.book
    worksheet = writer.sheets['Sheet1']
    
    # Create a bar chart object
    chart = workbook.add_chart({'type': 'bar'})

    # Configure the series of the chart from the DataFrame data. 
    # Here we need to adjust the cell range based on the DataFrame's size
    chart.add_series({
        'categories': '=Sheet1!$A$2:$A$5',
        'values':     '=Sheet1!$B$2:$B$5',
    })

    # Optionally, add chart title, axis titles, etc.
    chart.set_title({'name': 'Values by Category'})
    chart.set_x_axis({'name': 'Category'})
    chart.set_y_axis({'name': 'Value'})

    # Insert the chart into the worksheet with an offset
    worksheet.insert_chart('D2', chart)

# Note: The Excel file 'data_with_chart.xlsx' is saved in your current directory.