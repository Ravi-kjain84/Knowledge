import pandas as pd
import nltk
from nltk.corpus import wordnet as wn

# Ensure you have the necessary NLTK data
nltk.download('wordnet')

def categorize_column(name):
    """
    Categorize column based on its name.
    """
    # Define simple keyword-based categories
    if 'id' in name.lower():
        return 'Identifier'
    elif 'customer' in name.lower():
        return 'Customer Related'
    elif 'currency' in name.lower() or 'ccy' in name.lower():
        return 'Currency Related'
    else:
        return 'Other'

def is_customer_related(name):
    """
    Determine if a column name is semantically related to 'customer' using NLP.
    """
    # Get synsets for the column name and 'customer'
    customer_synsets = wn.synsets('customer')
    column_synsets = wn.synsets(name)
    
    # Check for semantic relation
    for col_syn in column_synsets:
        for cust_syn in customer_synsets:
            if col_syn.wup_similarity(cust_syn) > 0.5:  # Adjust threshold as needed
                return True
    return False

# Example DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, None], 'account_balance': [None, 200, 300]})
df2 = pd.DataFrame({'user_name': [4, 5, 6], 'currency_value': [None, None, 9], 'ccy_code': [10, 11, 12]})

# Store your DataFrames in a dictionary for easy iteration
dataframes = {'df1': df1, 'df2': df2}

# Initialize an empty list to store each row of metadata
metadata = []

# Iterate through each DataFrame to collect its metadata
for name, df in dataframes.items():
    for col in df.columns:
        # Calculate the number of nulls and null percentage for each column
        total_rows = len(df)
        null_values = df[col].isnull().sum()
        null_percentage = (null_values / total_rows) * 100
        
        # Determine the category of the column
        category = categorize_column(col)
        
        # Check if the column is customer-related using NLP
        customer_related = 'Yes' if is_customer_related(col) else 'No'
        
        # Append the metadata for this column to the list
        metadata.append({
            'DataFrame': name,
            'Column': col,
            'DataType': df[col].dtype,
            'TotalRows': total_rows,
            'NullValues': null_values,
            'NullPercentage': null_percentage,
            'Category': category,
            'IsCustomerRelated': customer_related
        })

# Convert the list of metadata into a DataFrame
metadata_df = pd.DataFrame(metadata)

print(metadata_df)
---------------------------------------------------

import pandas as pd

def categorize_column(name):
    """
    Categorize column based on its name using predefined keywords.
    """
    # Define keywords for each category
    identifier_keywords = ['id']
    customer_related_keywords = ['customer', 'client', 'user']
    currency_related_keywords = ['currency', 'ccy']
    
    # Convert name to lowercase for case-insensitive comparison
    name_lower = name.lower()
    
    # Check each category
    if any(keyword in name_lower for keyword in identifier_keywords):
        return 'Identifier'
    elif any(keyword in name_lower for keyword in customer_related_keywords):
        return 'Customer Related'
    elif any(keyword in name_lower for keyword in currency_related_keywords):
        return 'Currency Related'
    else:
        return 'Other'

# Example DataFrames as before
df1 = pd.DataFrame({'customer_id': [1, 2, None], 'account_balance': [None, 200, 300]})
df2 = pd.DataFrame({'user_name': [4, 5, 6], 'currency_value': [None, None, 9], 'ccy_code': [10, 11, 12]})

dataframes = {'df1': df1, 'df2': df2}
metadata = []

for name, df in dataframes.items():
    for col in df.columns:
        total_rows = len(df)
        null_values = df[col].isnull().sum()
        null_percentage = (null_values / total_rows) * 100
        category = categorize_column(col)
        
        metadata.append({
            'DataFrame': name,
            'Column': col,
            'DataType': df[col].dtype,
            'TotalRows': total_rows,
            'NullValues': null_values,
            'NullPercentage': null_percentage,
            'Category': category
        })

metadata_df = pd.DataFrame(metadata)

print(metadata_df)

___________________________________________________

import pandas as pd
import spacy

# Load the spaCy English language model
nlp = spacy.load("en_core_web_sm")

# Define a function to check if a column name is related to customer details
def is_customer_related(column_name):
    # List of customer related words to compare with
    customer_related_words = ['customer', 'client', 'account']
    customer_nlp = [nlp(word) for word in customer_related_words]
    
    # Process the column name with spaCy
    column_nlp = nlp(column_name.replace('_', ' '))
    
    # Check for semantic similarity
    for token in column_nlp:
        for customer_token in customer_nlp:
            if token.similarity(customer_token) > 0.5:  # Threshold for similarity
                return True
    return False

# Example DataFrame
df = pd.DataFrame({
    'customer_id': [1, 2, 3],
    'account_balance': [100, 200, 300],
    'transaction_date': ['2021-01-01', '2021-01-02', '2021-01-03']
})

# Identify customer related columns
customer_related_columns = [col for col in df.columns if is_customer_related(col)]

print("Customer related columns:", customer_related_columns)
--------------------------------------------

import pandas as pd
import spacy

# Load the spaCy English language model
nlp = spacy.load("en_core_web_sm")

# Define a function to check if a column name is related to customer details
def is_customer_related(column_name):
    # List of customer related words to compare with
    customer_related_words = ['customer', 'client', 'account']
    customer_nlp = [nlp(word) for word in customer_related_words]
    
    # Process the column name with spaCy
    column_nlp = nlp(column_name.replace('_', ' '))
    
    # Check for semantic similarity
    for token in column_nlp:
        for customer_token in customer_nlp:
            if token.similarity(customer_token) > 0.5:  # Threshold for similarity
                return True
    return False

# Your DataFrame (example)
df = pd.DataFrame({
    'customer_id': [1, 2, 3],
    'account_balance': [100, 200, 300],
    'transaction_date': ['2021-01-01', '2021-01-02', '2021-01-03'],
    'user_email': ['email1@example.com', 'email2@example.com', 'email3@example.com']
})

# Initialize an empty list to store each column's metadata
metadata = []

# Iterate through each column in the DataFrame
for col in df.columns:
    # Preprocess the column name by replacing underscores with spaces
    processed_col_name = col.replace('_', ' ')
    
    # Determine if the column is customer-related
    customer_related = is_customer_related(col)
    
    # Append the metadata for this column to the list
    metadata.append({
        'Column': col,
        'IsCustomerRelated': customer_related
    })

# Convert the list of metadata into a DataFrame
metadata_df = pd.DataFrame(metadata)

# Display the metadata DataFrame
print(metadata_df)
-----------------------------------------------------------------

customer_related_words = [
    "customer", "client", "account", "user", "cardholder", "holder", "owner",
    "subscriber", "member", "clientele", "consumer", "patron", "depositor",
    "borrower", "lender", "creditor", "debtor", "saver", "investor",
    "beneficiary", "name", "surname", "lastname", "firstname", "address",
    "email", "phone", "contact", "mobile", "zipcode", "postal", "city",
    "state", "country", "nationality", "dob", "birthdate", "gender",
    "marital", "occupation", "employment", "employer", "income", "salary",
    "transaction", "balance", "deposit", "withdrawal", "transfer", "payment",
    "statement", "loan", "mortgage", "credit", "debit", "asset", "liability",
    "portfolio", "risk", "rating", "score", "product", "service", "offer",
    "promotion", "campaign", "feedback", "complaint", "inquiry", "interaction",
    "relationship", "tenure", "history", "activity", "behavior", "preference",
    "interest", "security", "question", "answer", "authentication",
    "verification", "identity", "profile", "risk_weighted", "assets",
    "liquidity", "capital_requirement", "exposure", "default", "loss",
    "provision", "impairment", "collateral", "derivative", "hedge",
    "valuation", "fair_value", "amortized_cost", "financial_instrument",
    "lease", "rental", "revenue", "expense", "IFRS", "GSIB", "FINREP",
    "regulatory", "compliance", "audit", "reporting", "benchmark", 
    "stress_test", "scenario_analysis", "solvency", "capital_adequacy",
    "leverage_ratio", "liquidity_coverage", "net_stable_funding", "risk_management",
    "due_diligence", "KYC", "AML", "anti_money_laundering", "know_your_customer",
    "sanction", "watchlist", "PEP", "politically_exposed_person", "fraud_detection",
    "credit_risk", "market_risk", "operational_risk", "liquidity_risk", 
    "systemic_risk", "reputational_risk", "strategic_risk", "compliance_risk"
]
------------------------------------------

import pandas as pd

# Example DataFrames
# You would replace these with your actual DataFrames
dataframes_list = [
    pd.DataFrame(columns=['first_name', 'last_column']),
    pd.DataFrame(columns=['second_test', 'another_column']),
]

# Step 1: Merge column names into a single list
all_column_names = [name for df in dataframes_list for name in df.columns]

# Step 2: Replace underscores and split the names into individual words
words = " ".join(all_column_names).replace('_', ' ').split()

# Step 3: Find unique words
unique_words = set(words)

# Step 4: Remove prepositions or proposition-related words
# Note: You might need to expand this list based on your requirements
prepositions = {'of', 'in', 'to', 'for', 'with', 'on', 'at', 'from', 'by', 'about', 'as', 'into', 'like', 'through', 'after', 'over', 'between', 'out', 'against', 'during', 'without', 'before', 'under', 'around', 'among'}
filtered_words = unique_words - prepositions

print(filtered_words)
----------------------

import pandas as pd

def identify_unique_words_from_column(df):
    # List of common English prepositions to exclude
    prepositions = {"above", "across", "against", "along", "among", "around", "at", "before", "behind", "below",
                    "beneath", "beside", "between", "by", "down", "during", "for", "from", "in", "into", "near",
                    "of", "off", "on", "to", "under", "up", "with", "without"}
    
    # Extracting words from the 'column_names' column, excluding prepositions
    words = [word for col_name in df['column_names'] for word in col_name.split('_') if word not in prepositions]
    
    # Counting occurrences of each word
    word_counts = pd.Series(words).value_counts().reset_index()
    
    # Renaming columns to 'Word' and 'Count'
    word_counts.columns = ['Word', 'Count']
    
    return word_counts

# Example usage:
# Assuming your DataFrame 'df' has a column 'column_names' containing the names
# df = pd.DataFrame({'column_names': ['column_name1', 'column_name2', ...]})
# result_df = identify_unique_words_from_column(df)
# print(result_df)

