from google.cloud import bigquery
import pandas as pd

# Initialize BigQuery client
client = bigquery.Client()

# Set your project ID and dataset ID
project_id = 'your_project_id'
dataset_id = 'your_dataset_id'

# Function to perform schema discovery
def schema_discovery():
    # Construct SQL query to fetch table and column information
    query = f"""
        SELECT
            table_name,
            column_name,
            data_type
        FROM
            `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        ORDER BY
            table_name,
            ordinal_position
    """
    # Execute the query
    query_job = client.query(query)
    # Fetch results
    schema_results = query_job.result()
    # Convert results to DataFrame for easy manipulation
    schema_df = pd.DataFrame(schema_results)
    return schema_df

# Function to perform data profiling
def data_profiling():
    # Construct SQL query to perform data profiling
    query = f"""
        SELECT
            table_name,
            column_name,
            data_type,
            COUNT(*) AS total_rows,
            COUNTIF(column_name IS NULL) AS null_count,
            COUNTIF(NOT column_name IS NULL) AS non_null_count,
            COUNT(DISTINCT column_name) AS distinct_count,
            MIN(column_name) AS min_value,
            MAX(column_name) AS max_value,
            APPROX_QUANTILES(column_name, 10)[OFFSET(5)] AS median_value,
            AVG(column_name) AS mean_value,
            STDDEV(column_name) AS stddev_value
        FROM
            `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        GROUP BY
            table_name,
            column_name,
            data_type
    """
    # Execute the query
    query_job = client.query(query)
    # Fetch results
    profiling_results = query_job.result()
    # Convert results to DataFrame for easy manipulation
    profiling_df = pd.DataFrame(profiling_results)
    return profiling_df

# Main function
def main():
    # Perform schema discovery
    print("Performing Schema Discovery...")
    schema_df = schema_discovery()
    print("\nSchema Discovery Results:")
    print(schema_df.head())

    # Perform data profiling
    print("\nPerforming Data Profiling...")
    profiling_df = data_profiling()
    print("\nData Profiling Results:")
    print(profiling_df.head())

if __name__ == "__main__":
    from google.cloud import bigquery
import pandas as pd

# Function to fetch schema for a given project and dataset
def fetch_schema(project_id, dataset_id):
    client = bigquery.Client()
    query = f"""
        SELECT
            table_name,
            column_name,
            data_type
        FROM
            `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        ORDER BY
            table_name,
            ordinal_position
    """
    query_job = client.query(query)
    schema_results = query_job.result()
    schema_df = pd.DataFrame(schema_results)
    return schema_df

# Function to compare schemas and find differences
def compare_schemas(projects):
    all_tables = {}
    for project_id, dataset_id in projects:
        schema_df = fetch_schema(project_id, dataset_id)
        tables = set(schema_df['table_name'])
        all_tables[project_id] = tables
    
    common_tables = set.intersection(*all_tables.values())
    unique_tables = {project_id: tables - common_tables for project_id, tables in all_tables.items()}
    
    print("Common Tables:")
    print(common_tables)
    print("\nUnique Tables:")
    for project_id, tables in unique_tables.items():
        print(f"{project_id}: {tables}")

# List of projects and datasets to compare
projects = [
    ('project1_id', 'dataset1_id'),
    ('project2_id', 'dataset2_id'),
    ('project3_id', 'dataset3_id'),
    ('project4_id', 'dataset4_id')
]

# Compare schemas and find differences
compare_schemas(projects)



