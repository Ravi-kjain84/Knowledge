from google.cloud import bigquery
import pandas as pd

# Function to fetch schema for a given project and dataset
def fetch_schema(project_id, dataset_id):
    client = bigquery.Client()
    query = f"""
        SELECT
            table_name,
            column_name,
            data_type
        FROM
            `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        ORDER BY
            table_name,
            ordinal_position
    """
    query_job = client.query(query)
    schema_results = query_job.result()
    schema_df = pd.DataFrame(schema_results)
    return schema_df

# Function to categorize tables based on naming patterns
def categorize_tables(table_names):
    categories = {
        'Source': set(),
        'Reference Data': set(),
        'Report': set(),
        'Intermediary': set()
    }

    for table_name in table_names:
        if table_name.lower().endswith('_source'):
            categories['Source'].add(table_name)
        elif '_rd_' in table_name.lower():
            categories['Reference Data'].add(table_name)
        elif table_name.lower().endswith('_report'):
            categories['Report'].add(table_name)
        else:
            categories['Intermediary'].add(table_name)

    return categories

# Function to compare schemas and find differences
def compare_schemas(projects):
    all_tables = {}
    for project_id, dataset_id in projects:
        schema_df = fetch_schema(project_id, dataset_id)
        tables = set(schema_df['table_name'])
        all_tables[project_id] = tables
    
    common_tables = set.intersection(*all_tables.values())
    unique_tables = {project_id: tables - common_tables for project_id, tables in all_tables.items()}
    
    comparison_results = {}
    for project_id, tables in unique_tables.items():
        categories = categorize_tables(tables)
        comparison_results[project_id] = categories
    
    return comparison_results

# List of projects and datasets to compare
projects = [
    ('project1_id', 'dataset1_id'),
    ('project2_id', 'dataset2_id'),
    ('project3_id', 'dataset3_id'),
    ('project4_id', 'dataset4_id')
]

# Compare schemas and find differences
comparison_results = compare_schemas(projects)

# Convert comparison results to DataFrame
df_dict = {}
for project_id, categories in comparison_results.items():
    for category, tables in categories.items():
        for table in tables:
            df_dict.setdefault('Project', []).append(project_id)
            df_dict.setdefault('Category', []).append(category)
            df_dict.setdefault('Table', []).append(table)

result_df = pd.DataFrame(df_dict)
print(result_df)


------------------------------------------------------------------------------------------------------------

from google.cloud import bigquery
import pandas as pd

# Function to fetch schema for a given project and dataset
def fetch_schema(project_id, dataset_id):
    client = bigquery.Client()
    query = f"""
        SELECT
            table_name,
            column_name,
            data_type
        FROM
            `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        ORDER BY
            table_name,
            ordinal_position
    """
    query_job = client.query(query)
    schema_results = query_job.result()
    schema_df = pd.DataFrame(schema_results)

    # Categorize tables
    table_names = set(schema_df['table_name'])
    categories = categorize_tables(table_names)

    # Add categories as a separate column
    schema_df['Category'] = schema_df['table_name'].apply(lambda table_name: get_table_category(table_name, categories))
    
    return schema_df

# Function to categorize tables based on naming patterns
def categorize_tables(table_names):
    categories = {
        'Source': set(),
        'Reference Data': set(),
        'Report': set(),
        'Intermediary': set()
    }

    for table_name in table_names:
        if table_name.lower().endswith('_source'):
            categories['Source'].add(table_name)
        elif '_rd_' in table_name.lower():
            categories['Reference Data'].add(table_name)
        elif table_name.lower().endswith('_report'):
            categories['Report'].add(table_name)
        else:
            categories['Intermediary'].add(table_name)

    return categories

# Function to get the category of a table
def get_table_category(table_name, categories):
    for category, tables in categories.items():
        if table_name in tables:
            return category
    return 'Uncategorized'

# Example usage
project_id = 'your_project_id'
dataset_id = 'your_dataset_id'

schema_df = fetch_schema(project_id, dataset_id)
print("Schema with Categories:")
print(schema_df.head())


-------------------------------------------------------------------------------------

from google.cloud import bigquery
import pandas as pd

# Function to fetch schema for a given project and dataset
def fetch_schema(project_datasets):
    client = bigquery.Client()
    all_dataframes = []

    for project_id, dataset_id in project_datasets:
        query = f"""
            SELECT
                table_name,
                column_name,
                data_type
            FROM
                `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
            ORDER BY
                table_name,
                ordinal_position
        """
        query_job = client.query(query)
        schema_results = query_job.result()
        schema_df = pd.DataFrame(schema_results)

        # Add project and dataset columns
        schema_df['Project'] = project_id
        schema_df['Dataset'] = dataset_id

        all_dataframes.append(schema_df)

    # Concatenate all dataframes
    result_df = pd.concat(all_dataframes, ignore_index=True)
    
    return result_df

# Example usage
project_datasets = [
    ('project1_id', 'dataset1_id'),
    ('project2_id', 'dataset2_id')
]

schema_df = fetch_schema(project_datasets)
print("Combined Schema:")
print(schema_df.head())

------------------------------------------------

 # Filter out tables from project 2 that already exist in project 1
    project1_tables = set(result_df[result_df['Project'] == project_datasets[0][0]]['table_name'])
    result_df = result_df[~((result_df['Project'] == project_datasets[1][0]) & (result_df['table_name'].isin(project1_tables)))]

-----------------------------------------------

from google.cloud import bigquery
import pandas as pd

# Function to fetch schema for a given project and dataset
def fetch_schema(project_datasets):
    client = bigquery.Client()
    all_dataframes = []

    # Fetch schema for project 1
    project1_id, dataset1_id = project_datasets[0]
    project1_schema = fetch_project_schema(client, project1_id, dataset1_id)
    all_dataframes.append(project1_schema)

    for project_id, dataset_id in project_datasets[1:]:
        # Fetch schema for the project
        schema_df = fetch_project_schema(client, project_id, dataset_id)
        all_dataframes.append(schema_df)

    # Concatenate all dataframes
    result_df = pd.concat(all_dataframes, ignore_index=True)

    # Remove duplicates based on project 1 schema
    project1_tables = set(project1_schema['table_name'])
    result_df = result_df[~result_df['table_name'].isin(project1_tables)]

    return result_df

# Function to fetch schema for a project and dataset
def fetch_project_schema(client, project_id, dataset_id):
    query = f"""
        SELECT
            table_name,
            column_name,
            data_type
        FROM
            `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        ORDER BY
            table_name,
            ordinal_position
    """
    query_job = client.query(query)
    schema_results = query_job.result()
    schema_df = pd.DataFrame(schema_results)

    # Add project and dataset columns
    schema_df['Project'] = project_id
    schema_df['Dataset'] = dataset_id

    return schema_df

# Example usage
project_datasets = [
    ('project1_id', 'dataset1_id'),
    ('project2_id', 'dataset2_id'),
    ('project3_id', 'dataset3_id')
]

schema_df = fetch_schema(project_datasets)
print("Combined Schema:")
print(schema_df.head())

----------------------------------

Yes, intermediary table details can be obtained via the Information_Schema table, a standard method for accessing database information in Google Cloud

To identify intermediary tables, we can:

Utilize Information_Schema for all table and column names.
Categorize tables as Source, Reference, or Output based on their names (Source tables end with _Source, Reference tables include RD, Output tables contain Output or Extract).
Exclude tables linked to the rules_engine.
Consider the rest as intermediary, potentially requiring further refinement.


The steps should be applied collectively to all environments—the common (radar52), RWA (rwapc-51), and liquidity (fotc51-prod) environments—and separately for the UK and other regions to account for SDI version variations.

Here's the sample table for gathering information through the previously mentioned method:

I have already developed the methodology for obtaining these details but plan to further analyze before sharing any data. If you have suggestions on the approach, please let me know. I'll finalize the information on intermediary tables and share it by Monday.
